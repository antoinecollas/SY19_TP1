---
title: "TP3"
output:
  pdf_document: default
  html_notebook: default
---

#Problème de classification
##Chargement des données
Nous commençons par charger les données du problème de classification. Ce problème de classification ne possède que 2 classes. 43% des données appratiennent à la classe 1. 57% appartiennent à la classe 2.

```{r}
data <- read.table('../tp3_a18_clas_app.txt')
nrow(data)
sum(data[,-1]==1) / nrow(data)
sum(data[,-1]==2) / nrow(data)
```


Ensuite nous découpons l'ensemble des données en 2 ensembles: un pour l'entrainement et un pour le test.  Nous répartissons les données entre les 2 ensembles de manière aléatoire afin que nos 2 ensembles aient les mêmes proportions des étiquettes que dans l'ensemble initial. Nous fixons le générateur de nombres aléatoires afin de conserver le même ensemble de test au cours des différentes executions de ce notebook.

```{r}
set.seed(123)
ratio_training <- 3/4
train <- sample(1:nrow(data), floor(nrow(data)*ratio_training))
data_train <- data[train,]
data_test <- data[-train,]
```

##Méthode d'évaluation
Nous évaluons chaque modèle sur l'ensemble d'entrainement à l'aide d'une validation croisée (avec 10 parties). Nous choisissons notre modèle comme étant celui minimisant l'erreur évaluée sur l'ensemble d'entrainement par validation croisée. Enfin, nous estimons l'erreur de notre modèle sur l'ensemble de test. L'erreur de classification est définie par la probabilité d’erreur.

##Analyse discriminante (linéaire, quadratique et classifieur bayésien naïf).

```{r}
#fonction réalisant la validation croisée avec K=10 (pour api similaire à lda)
CV_eval <- function(model, data){
  K <- 10
  n <- nrow(data)
  folds <- sample(1:K,n,replace=TRUE)
  CV <- 0
  for(k in (1:K)){
    model.fit <- model(formula=as.factor(y)~., data=data[folds!=k,])
    pred <- predict(model.fit, data[folds==k,])
    CV <- CV+sum(data$y[folds==k]!=pred$class)
  }
  CV <- CV/n
  return(CV)
}
```

```{r}
#fonction réalisant 10 évaluations croisées avec K=10 (pour naiveBayes)
CV_eval_naiveBayes <- function(model, data){
  K <- 10
  n <- nrow(data)
  folds <- sample(1:K,n,replace=TRUE)
  CV <- 0
  for(k in (1:K)){
    model.fit <- model(formula=as.factor(y)~., data=data[folds!=k,])
    pred <- predict(model.fit, data[folds==k,])
    CV <- CV+sum(data$y[folds==k]!=pred)
  }
  CV <- CV/n
  return(CV)
}
```

```{r}
require(MASS)
#validation croisée sur l'ensemble d'entrainement
CV_eval(lda, data_train) # 30% d'erreur
CV_eval(qda, data_train) # 40% d'erreur
library(e1071)
CV_eval_naiveBayes(naiveBayes, data_train) # 15% d'erreur
```
```{r}
#essayer Regularized discriminant analysis
```

```{r}
#Régression logistique
```

```{r}
#fonction réalisant 10 évaluations croisées avec K=10 (pour régression logistique)
CV_eval_glm <- function(model, data){
  K <- 10
  n <- nrow(data)
  folds <- sample(1:K,n,replace=TRUE)
  CV <- 0
  for(k in (1:K)){
    glm.fit <- glm(y~., data=data[folds!=k,], family=binomial)
    pred <- predict(glm.fit, newdata=data[folds==k,], type='response')>0.5
    CV <- CV+sum(data$y[folds==k]!=pred)
  }
  CV <- CV/n
  return(CV)
}
```

```{r}
#validation croisée sur l'ensemble d'entrainement avec une régression logistique
data_train_glm <- data_train
data_train_glm$y <- data_train_glm$y - 1
CV_eval_glm(glm, data_train_glm)  # 30% d'erreur
```


```{r}
#ROC
```


```{r}
#ROC
```