---
title: "TP3"
output:
  pdf_document: default
  html_notebook: default
---

#Problème de classification
##Chargement des données
Nous commençons par charger les données du problème de classification. Ce problème de classification ne possède que 2 classes. 43% des individus appratiennent à la classe 1. 57% appartiennent à la classe 2. Ce jeu de données contient 200 individus, chacun est décrit par 36 variables.

```{r}
full_data <- read.table('../tp3_a18_clas_app.txt')
nrow(full_data)
sum(full_data[,-1]==1) / nrow(full_data)
sum(full_data[,-1]==2) / nrow(full_data)
```


Ensuite nous découpons l'ensemble des données en 2 ensembles: un pour l'entrainement et un pour le test.  Nous répartissons les données entre les 2 ensembles de manière aléatoire afin que nos 2 ensembles aient les mêmes proportions des étiquettes que dans l'ensemble initial. Nous fixons le générateur de nombres aléatoires afin de conserver le même ensemble de test au cours des différentes executions de ce notebook.

```{r}
set.seed(123)
ratio_training <- 3/4
train <- sample(1:nrow(full_data), floor(nrow(full_data)*ratio_training))
data_train <- full_data[train,]
data_test <- full_data[-train,]
```

##Méthode d'évaluation
Nous évaluons chaque modèle sur l'ensemble d'entrainement à l'aide d'une validation croisée (avec 10 parties). Nous calculons l'erreur moyenne et l'écart type. Nous appliquons la "One-standard-error rule" pour choisir notre modèle final. Enfin, nous estimons l'erreur de notre modèle sur l'ensemble de test. L'erreur de classification est définie par la probabilité d’erreur.

##Validation croisée
Nous avons implémenté une fonction qui calcule l'erreur de classification et l'écart type par validation croisée pour l'ensemble des modèles que nous utiliserons par la suite.
```{r}
source('CV.R')
```

##Analyse discriminante (linéaire, quadratique et classifieur bayésien naïf).
Nous commençons par utiliser les analyses discriminantes linéaire et quadratique ainsi que le classifieur bayésien naif.
```{r}
set.seed(123)
#validation croisée sur l'ensemble d'entrainement
res <- data.frame("Erreur"=rep(0,3), "Ecart type"=rep(0,3))
row.names(res) <- c("ADL", "ADQ", "Bayésien naif")
res[1,] <- CV_eval('adl', data_train) #analyse discriminantes linéaire
res[2,] <- CV_eval('adq', data_train) #analyse discriminantes quadratique
res[3,] <- CV_eval('bayesien_naif', data_train)
```
Nous obtenons les erreurs suivantes:

```{r, echo=FALSE}
knitr::kable(res)
```

## Analyse discriminante régularisée
Dans cette partie nous utilisons une analyse discriminante régularisée implémentée dans le package **klaR**. Cette implémentation permet d'obtenir des modèles régularisés de l'analyse disciminante linéaire, de l'analyse discriminante quadratique et du classifieur bayésien naïf. L'impléméntation du package **klaR** cherche les hyperparamètres $\gamma$ et $\lambda$ à l'aide d'une validation croisée sur l'ensemble d'entrainement. Les hyperparamètres optimaux trouvés sont $\gamma=0.86$ et $\lambda=0.04$. D'après la [documentation du package klaR](https://cran.r-project.org/web/packages/klaR/klaR.pdf) (page 58), cela correspond à des matrices de variances-covariances dont les termes de chaque diagonale sont quasiement égaux et les éléments en dehors de la diagonale sont proches de 0. Ce classifieur correspond à un classifieur bayésien naïf régularisé (par les termes des diagonales de matrices de variances-covariances égaux). L'erreur trouvée est proche de celle trouvée du classifieur bayésien naïf.

```{r}
require(klaR)
set.seed(123)
model <- rda(formula=as.factor(y)~., data=data_train, crossval=TRUE, fold=10)
model$regularization
model$error.rate[2]
```

## Régression logistique
Nous testons la régression logistique bien que ce modèle soit similaire à l'analyse discriminante linéaire (qui ne diffère que par l'évaluation des poids).

```{r}
set.seed(123)
#validation croisée sur l'ensemble d'entrainement avec une régression logistique
data_train_glm <- data_train
data_train_glm$y <- data_train_glm$y - 1
res <- data.frame("Erreur"=0, "Ecart type"=0)
row.names(res) <- "Regression logistique"
res[1,] <- suppressWarnings(CV_eval('reg_logistique', data_train_glm))
```
Comme attendu, nous obtenons une erreur très proche de ADL:
```{r, echo=FALSE}
knitr::kable(res)
```
```{r}
#Extraction des prédicteurs intéressants (Factor discriminant analysis, PCA, corrélations, corrélations fournies pas la régression logistique, ...)
```

```{r}
#ROC des différents modèles
#matrice de confusion du meilleur modèle
```

# Problème de régression
## Chargement des données
Nous commençons par charger les données du problème de régression. Ce problème contient 200 individus (comme le problème de classification). Chaque individu est decrit par 50 variables quantitatives. La variable nommée "y" est la variable à prédire.

```{r}
rm(list=ls())
full_data <- read.table('../tp3_a18_reg_app.txt')
nrow(full_data)
```


Ensuite nous découpons l'ensemble des données en 2 ensembles: un pour l'entrainement et un pour le test. Nous fixons le générateur de nombres aléatoires afin de conserver le même ensemble de test au cours des différentes executions de ce notebook.

```{r}
set.seed(123)
ratio_training <- 3/4
train <- sample(1:nrow(full_data), floor(nrow(full_data)*ratio_training))
data_train <- full_data[train,]
data_test <- full_data[-train,]
```

## Méthode d'évaluation
Pour ce problème l'erreur de régression est définie par l'espérance de l'erreur quadratique. Nous estimons cette erreur et son écart type pour chaque modèle sur l'ensemble d'entrainement à l'aide d'une validation croisée (avec 10 parties). Ensuite, nous appliquons la "One-standard-error rule" pour choisir notre modèle final. Enfin, nous estimons l'erreur de notre modèle sur l'ensemble de test. 

## Régression linéaire
```{r}
reg <- lm(formula = y~., data=data_train)
res <- resid(reg)
```

### Analyse des résidus
Nous commençons par vérifier les hypothèses sur les résidus de la régression linéaire sur l'ensemble d'entrainement. Tout d'abord nous testons la normalité des résidus à l'aide d'un test de Shapiro sur les résidus.
```{r}
shapiro.test(res)
```
La p-value obtenue est très élevée, 0.824, les résidus sont donc probablement gaussiens.
Nous pouvons confirmer ce résultat à l'aide d'un diagramme Q-Q des quantiles des résidus standardisés par rapport à ceux d'une loi normale.
```{r}
plot(reg, which=2)
```

La quasi totalité des points sont situés sur la première diagonale, ce qui confirme que les résidus sont gaussiens.

De plus, nous vérifions l'homoscédasticité des résidus. Pour cela nous représentons graphiquement les résidus en fonction des valeurs prédites.

```{r}
res <- resid(reg)
plot(data_train$y, res)
abline(0,0)
```

Nous observons que la variance est la même quelque soit y. L'hypothèse d'homoscédasticité est donc vérifiée.

### Analyse de la stabilité
Ensuite, nous cherchons si certaines observations ont une forte influence sur les coefficients de la régression linéaire.
Deux types de points ont cet effet: ceux à effet de levier et ceux à fort résidu. Pour cela nous calculons la distance de Cook qui mesure l'effet de la suppression d'un point. Les points qui ont une distance de Cook supérieure à 1 sont considérés comme atypique.
```{r}
sum(cooks.distance(reg)>1)
```
Les données ont toutes des distances de Cook inférieures à 1, il n'y a donc a priori pas de point atypique.

### Calcul de l'erreur par validation croisée
Nous calculons l'erreur par validation croisée
```{r}
source('CV.R')
```

```{r}
set.seed(123)
#validation croisée sur l'ensemble d'entrainement
res <- data.frame("Erreur"=rep(0,1), "Ecart type"=rep(0,1))
row.names(res) <- c("Regression linéaire")
res[1,] <- CV_eval('reg_lineaire', data_train)
```
Nous obtenons l'erreur suivante:

```{r, echo=FALSE}
knitr::kable(res)
```

## Régression linéaire régularisées (ridge et lasso)

Etant le grand nombre de variables (50) pour le faible nombre de d'individus (200), nous cherchons à pénaliser les variables (faire tendre leurs coefficients vers 0) qui ont peu d'effet sur la valeur à prédire. Pour cela nous utilisons les modèles ridge et lasso qui ajoutent des termes de penalisation dans le problème de minimisation des moindres carrés.

```{r}
xapp <- model.matrix(y~., data_train)[,2:51]
yapp <- data_train$y
```

### Ridge
Nous commençons par chercher l'hyperparamètre lambda du modèle ridge. Nous le cherchons à l'aide d'une validation croisée sur l'ensemble d'entrainement. Le modèle ridge ainsi que la validation croisée sont implémentés dans le package **glmnet**. Du fait du terme de pénalisation, ridge (tout comme lasso) nécessite de centrer et réduire les variables (paramètre standardize dans glmnet).

```{r}
library(glmnet)
res <- data.frame("Erreur"=rep(0,1), "Ecart type"=rep(0,1))
row.names(res) <- c("Ridge")
cv.out <- cv.glmnet(xapp, yapp, alpha=0, lambda=exp(0:20), type.measure='mse', nfolds=10, standardize=TRUE)
#cv.out$lambda.min
#cv.out
hyperparametres <- list(alpha=0, lambda = cv.out$lambda.min)
res[1,] <- CV_eval('ridge_lasso', data_train, hyperparametres)
plot(cv.out)
```
Nous obtenons l'erreur suivante:

```{r, echo=FALSE}
knitr::kable(res)
```

L'erreur est très proche de l'erreur de la régression linéaire simple. La pénalisation ridge n'améliore pas la performance de notre modèle.

### Lasso
Nous commençons par chercher l'hyperparamètre lambda du modèle lasso. Nous le cherchons à l'aide d'une validation croisée sur l'ensemble d'entrainement. Le modèle lasso ainsi que la validation croisée sont implémentés dans le package **glmnet**.

```{r}
res <- data.frame("Erreur"=rep(0,1), "Ecart type"=rep(0,1))
row.names(res) <- c("Lasso")
cv.out <- cv.glmnet(xapp, yapp, alpha=1, lambda=NULL, type.measure='mse', nfolds=10, standardize=TRUE)
#cv.out$lambda.min
#cv.out
hyperparametres <- list(alpha=1, lambda=cv.out$lambda.min)
res[1,] <- CV_eval('ridge_lasso', data_train, hyperparametres)
plot(cv.out)
```
```{r, echo=FALSE}
knitr::kable(res)
```

```{r}
#graph lasso en fonction de lambda
```

```{r}
#tracer y_hat en fonction de y por le modèle retenu
```
